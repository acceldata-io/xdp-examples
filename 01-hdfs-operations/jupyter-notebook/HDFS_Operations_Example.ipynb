{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# HDFS Operations with Spark - Acceldata Examples\n",
        "\n",
        "This notebook demonstrates comprehensive HDFS read and write operations using Apache Spark.\n",
        "\n",
        "**Company**: Acceldata Inc. (acceldata.io)  \n",
        "**Version**: 1.0.0  \n",
        "**Author**: Acceldata Platform Team\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook covers:\n",
        "1. Setting up Spark session for HDFS operations\n",
        "2. Writing data to HDFS in various formats\n",
        "3. Reading data from HDFS\n",
        "4. Data analysis and transformations\n",
        "5. Best practices for HDFS operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "\n",
        "# Spark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
        "from pyspark.sql.functions import col, when, avg, count, max as spark_max, min as spark_min\n",
        "\n",
        "# Configure display options\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "print(f\"Notebook started at: {datetime.now()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Initialize Spark Session\n",
        "\n",
        "First, let's create a Spark session configured for HDFS operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "HDFS_NAMENODE = \"hdfs://localhost:9000\"  # Update this to your HDFS namenode\n",
        "BASE_PATH = \"/data/acceldata-examples\"\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HDFS Operations Notebook - Acceldata\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.hadoop.fs.defaultFS\", HDFS_NAMENODE) \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Set log level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"✓ Spark session created successfully\")\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"HDFS NameNode: {HDFS_NAMENODE}\")\n",
        "print(f\"Base Path: {BASE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Create Sample Data\n",
        "\n",
        "Let's create sample datasets that we'll use for our HDFS operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_employee_dataframe():\n",
        "    \"\"\"Create sample employee DataFrame\"\"\"\n",
        "    schema = StructType([\n",
        "        StructField(\"id\", IntegerType(), False),\n",
        "        StructField(\"name\", StringType(), False),\n",
        "        StructField(\"email\", StringType(), False),\n",
        "        StructField(\"department\", StringType(), False),\n",
        "        StructField(\"salary\", DoubleType(), False),\n",
        "        StructField(\"hire_date\", TimestampType(), False)\n",
        "    ])\n",
        "    \n",
        "    data = [\n",
        "        (1, \"John Doe\", \"john.doe@acceldata.io\", \"Engineering\", 75000.0, datetime(2023, 1, 15)),\n",
        "        (2, \"Jane Smith\", \"jane.smith@acceldata.io\", \"Sales\", 82000.0, datetime(2023, 2, 20)),\n",
        "        (3, \"Bob Johnson\", \"bob.johnson@acceldata.io\", \"Engineering\", 68000.0, datetime(2023, 3, 10)),\n",
        "        (4, \"Alice Brown\", \"alice.brown@acceldata.io\", \"Marketing\", 91000.0, datetime(2023, 4, 5)),\n",
        "        (5, \"Charlie Wilson\", \"charlie.wilson@acceldata.io\", \"HR\", 77500.0, datetime(2023, 5, 12)),\n",
        "        (6, \"Diana Prince\", \"diana.prince@acceldata.io\", \"Engineering\", 85000.0, datetime(2023, 6, 18)),\n",
        "        (7, \"Frank Miller\", \"frank.miller@acceldata.io\", \"Sales\", 72000.0, datetime(2023, 7, 22)),\n",
        "        (8, \"Grace Lee\", \"grace.lee@acceldata.io\", \"Marketing\", 88000.0, datetime(2023, 8, 30))\n",
        "    ]\n",
        "    \n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "def create_sales_dataframe():\n",
        "    \"\"\"Create sample sales DataFrame\"\"\"\n",
        "    schema = StructType([\n",
        "        StructField(\"transaction_id\", IntegerType(), False),\n",
        "        StructField(\"employee_id\", IntegerType(), False),\n",
        "        StructField(\"product\", StringType(), False),\n",
        "        StructField(\"amount\", DoubleType(), False),\n",
        "        StructField(\"transaction_date\", TimestampType(), False),\n",
        "        StructField(\"region\", StringType(), False)\n",
        "    ])\n",
        "    \n",
        "    products = [\"Data Platform\", \"Analytics Suite\", \"ML Tools\", \"Monitoring\"]\n",
        "    regions = [\"North\", \"South\", \"East\", \"West\"]\n",
        "    \n",
        "    data = []\n",
        "    for i in range(50):\n",
        "        data.append((\n",
        "            i + 1,\n",
        "            random.randint(1, 8),  # employee_id\n",
        "            random.choice(products),\n",
        "            round(random.uniform(1000, 50000), 2),\n",
        "            datetime.now() - timedelta(days=random.randint(1, 365)),\n",
        "            random.choice(regions)\n",
        "        ))\n",
        "    \n",
        "    return spark.createDataFrame(data, schema)\n",
        "\n",
        "# Create sample DataFrames\n",
        "employee_df = create_employee_dataframe()\n",
        "sales_df = create_sales_dataframe()\n",
        "\n",
        "print(\"✓ Sample DataFrames created\")\n",
        "print(f\"Employee records: {employee_df.count()}\")\n",
        "print(f\"Sales records: {sales_df.count()}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nEmployee Data Sample:\")\n",
        "employee_df.show(5)\n",
        "\n",
        "print(\"\\nSales Data Sample:\")\n",
        "sales_df.show(5)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Write Data to HDFS\n",
        "\n",
        "Now let's write our sample data to HDFS in various formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define output paths\n",
        "output_base = f\"{BASE_PATH}/output\"\n",
        "\n",
        "print(\"=== Writing Data to HDFS ===\")\n",
        "\n",
        "# 1. Write as Parquet (recommended for analytics)\n",
        "print(\"1. Writing employee data as Parquet...\")\n",
        "employee_df.write.mode(\"overwrite\").parquet(f\"{output_base}/employees_parquet\")\n",
        "print(\"   ✓ Employee data written as Parquet\")\n",
        "\n",
        "# 2. Write as CSV\n",
        "print(\"2. Writing employee data as CSV...\")\n",
        "employee_df.coalesce(1).write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(f\"{output_base}/employees_csv\")\n",
        "print(\"   ✓ Employee data written as CSV\")\n",
        "\n",
        "# 3. Write as JSON\n",
        "print(\"3. Writing employee data as JSON...\")\n",
        "employee_df.coalesce(1).write.mode(\"overwrite\") \\\n",
        "    .json(f\"{output_base}/employees_json\")\n",
        "print(\"   ✓ Employee data written as JSON\")\n",
        "\n",
        "# 4. Write with partitioning (by department)\n",
        "print(\"4. Writing employee data with partitioning...\")\n",
        "employee_df.write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"department\") \\\n",
        "    .parquet(f\"{output_base}/employees_partitioned\")\n",
        "print(\"   ✓ Employee data written with partitioning\")\n",
        "\n",
        "# 5. Write sales data with compression\n",
        "print(\"5. Writing sales data with compression...\")\n",
        "sales_df.write.mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"gzip\") \\\n",
        "    .parquet(f\"{output_base}/sales_compressed\")\n",
        "print(\"   ✓ Sales data written with compression\")\n",
        "\n",
        "print(\"\\n✓ All data successfully written to HDFS\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Read Data from HDFS\n",
        "\n",
        "Let's read the data back from HDFS and verify it was written correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Reading Data from HDFS ===\")\n",
        "\n",
        "# 1. Read Parquet data\n",
        "print(\"1. Reading employee data from Parquet...\")\n",
        "employees_parquet = spark.read.parquet(f\"{output_base}/employees_parquet\")\n",
        "print(f\"   Records read: {employees_parquet.count()}\")\n",
        "print(\"   Schema:\")\n",
        "employees_parquet.printSchema()\n",
        "\n",
        "# 2. Read CSV data\n",
        "print(\"\\n2. Reading employee data from CSV...\")\n",
        "employees_csv = spark.read.option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(f\"{output_base}/employees_csv\")\n",
        "print(f\"   Records read: {employees_csv.count()}\")\n",
        "employees_csv.show(3)\n",
        "\n",
        "# 3. Read JSON data\n",
        "print(\"3. Reading employee data from JSON...\")\n",
        "employees_json = spark.read.json(f\"{output_base}/employees_json\")\n",
        "print(f\"   Records read: {employees_json.count()}\")\n",
        "\n",
        "# 4. Read partitioned data\n",
        "print(\"4. Reading partitioned employee data...\")\n",
        "employees_partitioned = spark.read.parquet(f\"{output_base}/employees_partitioned\")\n",
        "print(f\"   Records read: {employees_partitioned.count()}\")\n",
        "print(\"   Partitions by department:\")\n",
        "employees_partitioned.groupBy(\"department\").count().show()\n",
        "\n",
        "# 5. Read compressed sales data\n",
        "print(\"5. Reading compressed sales data...\")\n",
        "sales_compressed = spark.read.parquet(f\"{output_base}/sales_compressed\")\n",
        "print(f\"   Records read: {sales_compressed.count()}\")\n",
        "\n",
        "print(\"\\n✓ All data successfully read from HDFS\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Data Analysis and Transformations\n",
        "\n",
        "Let's perform some data analysis on our HDFS data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Data Analysis ===\")\n",
        "\n",
        "# 1. Employee salary analysis\n",
        "print(\"1. Employee Salary Analysis:\")\n",
        "salary_stats = employees_parquet.select(\"salary\").describe()\n",
        "salary_stats.show()\n",
        "\n",
        "# 2. Department-wise analysis\n",
        "print(\"2. Department-wise Analysis:\")\n",
        "dept_analysis = employees_parquet.groupBy(\"department\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"employee_count\"),\n",
        "        avg(\"salary\").alias(\"avg_salary\"),\n",
        "        spark_max(\"salary\").alias(\"max_salary\"),\n",
        "        spark_min(\"salary\").alias(\"min_salary\")\n",
        "    ) \\\n",
        "    .orderBy(\"avg_salary\", ascending=False)\n",
        "\n",
        "dept_analysis.show()\n",
        "\n",
        "# 3. Sales analysis\n",
        "print(\"3. Sales Analysis:\")\n",
        "sales_analysis = sales_compressed.groupBy(\"region\", \"product\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"transaction_count\"),\n",
        "        avg(\"amount\").alias(\"avg_amount\"),\n",
        "        sum(\"amount\").alias(\"total_amount\")\n",
        "    ) \\\n",
        "    .orderBy(\"total_amount\", ascending=False)\n",
        "\n",
        "sales_analysis.show()\n",
        "\n",
        "# 4. Create derived columns\n",
        "print(\"4. Creating derived columns:\")\n",
        "employees_enhanced = employees_parquet.withColumn(\n",
        "    \"salary_category\",\n",
        "    when(col(\"salary\") < 70000, \"Low\")\n",
        "    .when(col(\"salary\") < 85000, \"Medium\")\n",
        "    .otherwise(\"High\")\n",
        ").withColumn(\n",
        "    \"years_of_service\",\n",
        "    (col(\"hire_date\").cast(\"long\") - datetime(2023, 1, 1).timestamp()) / (365.25 * 24 * 3600)\n",
        ")\n",
        "\n",
        "employees_enhanced.select(\"name\", \"department\", \"salary\", \"salary_category\").show()\n",
        "\n",
        "print(\"\\n✓ Data analysis completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Advanced HDFS Operations\n",
        "\n",
        "Let's explore some advanced HDFS operations and best practices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Advanced HDFS Operations ===\")\n",
        "\n",
        "# 1. Join operations across HDFS datasets\n",
        "print(\"1. Performing JOIN operations:\")\n",
        "joined_data = employees_parquet.alias(\"emp\") \\\n",
        "    .join(sales_compressed.alias(\"sales\"), col(\"emp.id\") == col(\"sales.employee_id\")) \\\n",
        "    .select(\n",
        "        col(\"emp.name\"),\n",
        "        col(\"emp.department\"),\n",
        "        col(\"sales.product\"),\n",
        "        col(\"sales.amount\"),\n",
        "        col(\"sales.region\")\n",
        "    )\n",
        "\n",
        "print(f\"   Joined records: {joined_data.count()}\")\n",
        "joined_data.show(10)\n",
        "\n",
        "# 2. Write joined results back to HDFS\n",
        "print(\"2. Writing joined results to HDFS...\")\n",
        "joined_data.write.mode(\"overwrite\") \\\n",
        "    .partitionBy(\"department\", \"region\") \\\n",
        "    .parquet(f\"{output_base}/employee_sales_joined\")\n",
        "print(\"   ✓ Joined data written to HDFS\")\n",
        "\n",
        "# 3. Aggregated reporting\n",
        "print(\"3. Creating aggregated reports:\")\n",
        "department_sales_report = joined_data.groupBy(\"department\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_transactions\"),\n",
        "        sum(\"amount\").alias(\"total_sales\"),\n",
        "        avg(\"amount\").alias(\"avg_transaction_amount\")\n",
        "    ) \\\n",
        "    .orderBy(\"total_sales\", ascending=False)\n",
        "\n",
        "department_sales_report.show()\n",
        "\n",
        "# Write report to HDFS\n",
        "department_sales_report.coalesce(1).write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(f\"{output_base}/department_sales_report\")\n",
        "print(\"   ✓ Department sales report written to HDFS\")\n",
        "\n",
        "# 4. Data quality checks\n",
        "print(\"4. Data Quality Checks:\")\n",
        "print(\"   Checking for null values in employee data:\")\n",
        "null_counts = employees_parquet.select([\n",
        "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in employees_parquet.columns\n",
        "])\n",
        "null_counts.show()\n",
        "\n",
        "print(\"   Checking salary ranges:\")\n",
        "salary_ranges = employees_parquet.select(\n",
        "    spark_min(\"salary\").alias(\"min_salary\"),\n",
        "    spark_max(\"salary\").alias(\"max_salary\"),\n",
        "    avg(\"salary\").alias(\"avg_salary\")\n",
        ")\n",
        "salary_ranges.show()\n",
        "\n",
        "print(\"\\n✓ Advanced operations completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Performance Optimization Tips\n",
        "\n",
        "Here are some best practices for optimizing HDFS operations with Spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Performance Optimization Examples ===\")\n",
        "\n",
        "# 1. Caching frequently accessed data\n",
        "print(\"1. Caching frequently accessed DataFrames:\")\n",
        "employees_parquet.cache()\n",
        "print(\"   ✓ Employee DataFrame cached in memory\")\n",
        "\n",
        "# Perform multiple operations on cached data\n",
        "print(\"   Performing multiple operations on cached data...\")\n",
        "count1 = employees_parquet.count()\n",
        "count2 = employees_parquet.filter(col(\"salary\") > 80000).count()\n",
        "print(f\"   Total employees: {count1}\")\n",
        "print(f\"   High salary employees: {count2}\")\n",
        "\n",
        "# 2. Optimal partitioning\n",
        "print(\"\\n2. Optimal partitioning strategies:\")\n",
        "print(f\"   Current partitions in employee data: {employees_parquet.rdd.getNumPartitions()}\")\n",
        "print(f\"   Current partitions in sales data: {sales_compressed.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Repartition for better performance\n",
        "sales_repartitioned = sales_compressed.repartition(4, \"region\")\n",
        "print(f\"   Sales data repartitioned by region: {sales_repartitioned.rdd.getNumPartitions()} partitions\")\n",
        "\n",
        "# 3. Coalescing for output optimization\n",
        "print(\"\\n3. Coalescing for output optimization:\")\n",
        "# Instead of many small files, create fewer larger files\n",
        "employees_parquet.coalesce(2).write.mode(\"overwrite\") \\\n",
        "    .parquet(f\"{output_base}/employees_coalesced\")\n",
        "print(\"   ✓ Employee data written with coalescing (fewer output files)\")\n",
        "\n",
        "# 4. Predicate pushdown example\n",
        "print(\"\\n4. Predicate pushdown optimization:\")\n",
        "# Filter early to reduce data movement\n",
        "high_salary_employees = spark.read.parquet(f\"{output_base}/employees_parquet\") \\\n",
        "    .filter(col(\"salary\") > 80000) \\\n",
        "    .select(\"name\", \"department\", \"salary\")\n",
        "\n",
        "print(f\"   High salary employees: {high_salary_employees.count()}\")\n",
        "high_salary_employees.show()\n",
        "\n",
        "# 5. Compression comparison\n",
        "print(\"\\n5. Compression comparison:\")\n",
        "# Write same data with different compression\n",
        "employees_parquet.write.mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(f\"{output_base}/employees_snappy\")\n",
        "\n",
        "employees_parquet.write.mode(\"overwrite\") \\\n",
        "    .option(\"compression\", \"gzip\") \\\n",
        "    .parquet(f\"{output_base}/employees_gzip\")\n",
        "\n",
        "print(\"   ✓ Data written with different compression formats\")\n",
        "print(\"   Note: Check HDFS to compare file sizes\")\n",
        "\n",
        "print(\"\\n✓ Performance optimization examples completed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Cleanup and Summary\n",
        "\n",
        "Let's clean up resources and summarize what we've accomplished.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Summary of HDFS Operations ===\")\n",
        "\n",
        "print(\"✓ Successfully demonstrated:\")\n",
        "print(\"  1. Spark session configuration for HDFS\")\n",
        "print(\"  2. Creating sample datasets\")\n",
        "print(\"  3. Writing data to HDFS in multiple formats:\")\n",
        "print(\"     - Parquet (recommended for analytics)\")\n",
        "print(\"     - CSV (for interoperability)\")\n",
        "print(\"     - JSON (for semi-structured data)\")\n",
        "print(\"     - Partitioned data (for performance)\")\n",
        "print(\"     - Compressed data (for storage efficiency)\")\n",
        "print(\"  4. Reading data from HDFS\")\n",
        "print(\"  5. Data analysis and transformations\")\n",
        "print(\"  6. Advanced operations (joins, aggregations)\")\n",
        "print(\"  7. Performance optimization techniques\")\n",
        "\n",
        "print(f\"\\nData written to HDFS base path: {BASE_PATH}\")\n",
        "print(\"Files created:\")\n",
        "print(\"  - employees_parquet/\")\n",
        "print(\"  - employees_csv/\")\n",
        "print(\"  - employees_json/\")\n",
        "print(\"  - employees_partitioned/\")\n",
        "print(\"  - sales_compressed/\")\n",
        "print(\"  - employee_sales_joined/\")\n",
        "print(\"  - department_sales_report/\")\n",
        "print(\"  - employees_coalesced/\")\n",
        "print(\"  - employees_snappy/\")\n",
        "print(\"  - employees_gzip/\")\n",
        "\n",
        "print(f\"\\nNotebook completed at: {datetime.now()}\")\n",
        "print(\"Company: Acceldata Inc. (acceldata.io)\")\n",
        "\n",
        "# Cleanup\n",
        "employees_parquet.unpersist()  # Remove from cache\n",
        "print(\"\\n✓ Resources cleaned up\")\n",
        "\n",
        "# Note: Keep Spark session running for interactive use\n",
        "# Uncomment the next line to stop Spark session\n",
        "# spark.stop()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
